{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“FinalRNN.ipynb”",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GitHub666666666/tangshi/blob/master/%E2%80%9CFinalRNN_ipynb%E2%80%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F104_A5-YbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import os\n",
        "\n",
        "start_token = 'G'\n",
        "end_token = 'E'\n",
        "batch_size = 64\n",
        "\n",
        "#第一个是参数名称，第二个参数是默认值，第三个是参数描述\n",
        "tf.app.flags.DEFINE_string('checkpoints_dir', './','descrip1')\n",
        "tf.app.flags.DEFINE_string('file_path', 'poems.txt','descrip2')\n",
        "tf.app.flags.DEFINE_string('model_prefix', 'poeam_geanerator','descrip3')\n",
        "tf.app.flags.DEFINE_integer('batch_size', 128,'descript4')\n",
        "tf.app.flags.DEFINE_float('learning_rate',0.01, 'descript5')\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "FLAGS = tf.app.flags.FLAGS\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1n-5xjq-Ybh",
        "colab_type": "text"
      },
      "source": [
        "#### 数据预处理部分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_7NPZLm-Ybj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_poems(file_name):\n",
        "    poems = []\n",
        "    with open(file_name, \"r\", encoding='utf-8', ) as f:\n",
        "        for line in f.readlines():\n",
        "            try:\n",
        "                title, content = line.strip().split(':')\n",
        "                content = content.replace(' ', '')\n",
        "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content or \\\n",
        "                                start_token in content or end_token in content:\n",
        "                    continue\n",
        "                if len(content) < 5 or len(content) > 80:\n",
        "                    continue\n",
        "                content = start_token + content + end_token\n",
        "                poems.append(content)\n",
        "            except ValueError as e:\n",
        "                pass\n",
        "    # 按诗的字数排序\n",
        "    poems = sorted(poems, key=lambda line: len(line))\n",
        "    # 统计每个字出现次数\n",
        "    all_words = []\n",
        "    for poem in poems:\n",
        "        all_words += [word for word in poem]  \n",
        "    counter = collections.Counter(all_words)  # 统计词和词频。\n",
        "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
        "    words, _ = zip(*count_pairs)\n",
        "    words = words[:len(words)] + (' ',)\n",
        "    word_int_map = dict(zip(words, range(len(words))))\n",
        "    poems_vector = [list(map(word_int_map.get, poem)) for poem in poems]\n",
        "    return poems_vector, word_int_map, words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRCoi-cA-Ybo",
        "colab_type": "text"
      },
      "source": [
        "### rnn_lstm model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtxuBTv5-Ybq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def rnn_model(model, input_data, output_data, vocab_size, rnn_size=128, num_layers=2, batch_size=64,\n",
        "              learning_rate=0.01):\n",
        "\n",
        "    end_points = {}\n",
        "    # 构建RNN基本单元RNNcell\n",
        "    if model == 'rnn':\n",
        "        cell_fun = tf.contrib.rnn.BasicRNNCell\n",
        "    elif model == 'gru':\n",
        "        cell_fun = tf.contrib.rnn.GRUCell\n",
        "    else:\n",
        "        cell_fun = tf.contrib.rnn.BasicLSTMCell\n",
        "\n",
        "    cell = cell_fun(rnn_size, state_is_tuple=True)\n",
        "    # 构建堆叠rnn，这里选用两层的rnn\n",
        "    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
        "    # 如果是训练模式，output_data不为None，则初始状态shape为[batch_size * rnn_size]\n",
        "    # 如果是生成模式，output_data为None，则初始状态shape为[1 * rnn_size]\n",
        "    if output_data is not None:\n",
        "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "    else:\n",
        "        initial_state = cell.zero_state(1, tf.float32)\n",
        "\n",
        "    # 构建隐层\n",
        "    with tf.device(\"/cpu:0\"):\n",
        "        embedding = tf.get_variable('embedding', initializer=tf.random_uniform(\n",
        "            [vocab_size + 1, rnn_size], -1.0, 1.0))\n",
        "        inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
        "\n",
        "    # [batch_size, ?, rnn_size] = [64, ?, 128]\n",
        "    outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n",
        "    output = tf.reshape(outputs, [-1, rnn_size])\n",
        "\n",
        "    weights = tf.Variable(tf.truncated_normal([rnn_size, vocab_size + 1]))\n",
        "    bias = tf.Variable(tf.zeros(shape=[vocab_size + 1]))\n",
        "    logits = tf.nn.bias_add(tf.matmul(output, weights), bias=bias)\n",
        "    # [?, vocab_size+1]\n",
        "\n",
        "    if output_data is not None:\n",
        "        # output_data must be one-hot encode\n",
        "        labels = tf.one_hot(tf.reshape(output_data, [-1]), depth=vocab_size + 1)\n",
        "        # should be [?, vocab_size+1]\n",
        "\n",
        "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "        # loss shape should be [?, vocab_size+1]\n",
        "        total_loss = tf.reduce_mean(loss)\n",
        "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
        "\n",
        "        end_points['initial_state'] = initial_state\n",
        "        end_points['output'] = output\n",
        "        end_points['train_op'] = train_op\n",
        "        end_points['total_loss'] = total_loss\n",
        "        end_points['loss'] = loss\n",
        "        end_points['last_state'] = last_state\n",
        "    else:\n",
        "        prediction = tf.nn.softmax(logits)\n",
        "\n",
        "        end_points['initial_state'] = initial_state\n",
        "        end_points['last_state'] = last_state\n",
        "        end_points['prediction'] = prediction\n",
        "\n",
        "    return end_points\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbEmPHEL-Ybt",
        "colab_type": "text"
      },
      "source": [
        "### 训练模型部分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiSMuqBu-Ybt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_training():\n",
        "    # 处理数据集\n",
        "    poems_vector, word_to_int, vocabularies = process_poems('./poems.txt')\n",
        "    # 生成batch\n",
        "    batches_inputs, batches_outputs = generate_batch(64, poems_vector, word_to_int)\n",
        "\n",
        "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
        "    output_targets = tf.placeholder(tf.int32, [batch_size, None])\n",
        "    # 构建模型\n",
        "    end_points = rnn_model(model='lstm', input_data=input_data, output_data=output_targets, vocab_size=len(\n",
        "        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=0.01)\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init_op)\n",
        "        for epoch in range(100):\n",
        "            n = 0\n",
        "            n_chunk = len(poems_vector) // batch_size\n",
        "            for batch in range(n_chunk):\n",
        "                loss, _, _ = sess.run([\n",
        "                    end_points['total_loss'],\n",
        "                    end_points['last_state'],\n",
        "                    end_points['train_op']\n",
        "                ], feed_dict={input_data: batches_inputs[n], output_targets: batches_outputs[n]})\n",
        "                n += 1\n",
        "                print('[INFO] Epoch: %d , batch: %d , training loss: %.6f' % (epoch, batch, loss))\n",
        "        saver.save(sess, './poem_generator')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5zzdySR-Ybx",
        "colab_type": "text"
      },
      "source": [
        "### 生成 诗歌部分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoYguk7r-Yby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_word(predict, vocabs):\n",
        "    t = np.cumsum(predict)\n",
        "    s = np.sum(predict)\n",
        "    # 该代码是作者写的，t的长度为vocab_size + 1, 随机生成一个数然后判断能插入第几个位置来取字\n",
        "    # 个人感觉这使得训练变得毫无意义\n",
        "    # sample = int(np.searchsorted(t, np.random.rand(1) * s))\n",
        "    # 而实际上输出的预测向量predict，随着训练过程应该逐渐向one-hot编码靠拢，所以应该取argmax函数\n",
        "    sample = np.argmax(predict)\n",
        "    if sample > len(vocabs):\n",
        "        sample = len(vocabs) - 1\n",
        "    return vocabs[sample]\n",
        "\n",
        "\n",
        "def gen_poem(begin_word):\n",
        "    batch_size = 1\n",
        "    print('[INFO] loading corpus from %s' % FLAGS.file_path)\n",
        "    poems_vector, word_int_map, vocabularies = process_poems(FLAGS.file_path)\n",
        "\n",
        "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
        "\n",
        "    end_points = rnn_model(model='lstm', input_data=input_data, output_data=None, vocab_size=len(\n",
        "        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init_op)\n",
        "\n",
        "        checkpoint = tf.train.latest_checkpoint(FLAGS.checkpoints_dir)\n",
        "        saver.restore(sess, checkpoint)\n",
        "\n",
        "        x = np.array([list(map(word_int_map.get, start_token))])\n",
        "\n",
        "        [predict, last_state] = sess.run([end_points['prediction'], end_points['last_state']],\n",
        "                                         feed_dict={input_data: x})\n",
        "        # 如果指定开始的字\n",
        "        if begin_word:\n",
        "            word = begin_word\n",
        "        # 如果不指定开始的字，就按根据start_token生成第一个字\n",
        "        else:\n",
        "            word = to_word(predict, vocabularies)\n",
        "        poem = ''\n",
        "        while word != end_token:\n",
        "            poem += word\n",
        "            x = np.zeros((1, 1))\n",
        "            # 比如，指定第一个字为“白”，则x就为x[[36]]，即batch_size为1，并且poems_length为1，生成下一个字\n",
        "            x[0, 0] = word_int_map[word]\n",
        "            # 传入input_data，此时没有output_data即为生成模式，并且传入初始状态为训练结束的状态\n",
        "            # state_shape为[1,rnn_size]\n",
        "            [predict, last_state] = sess.run([end_points['prediction'], end_points['last_state']],\n",
        "                                             feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
        "            # 根据预测结果生成对应的字\n",
        "            word = to_word(predict, vocabularies)\n",
        "        return poem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o914wgm-Yb2",
        "colab_type": "text"
      },
      "source": [
        "### 其他的一些处理函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvr2uEgX-Yb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(batch_size, poems_vec, word_to_int):\n",
        "    # 每次取64首诗进行训练\n",
        "    n_chunk = len(poems_vec) // batch_size\n",
        "    x_batches = []\n",
        "    y_batches = []\n",
        "    for i in range(n_chunk):\n",
        "        start_index = i * batch_size\n",
        "        end_index = start_index + batch_size\n",
        "\n",
        "        batches = poems_vec[start_index:end_index]\n",
        "        # 找到这个batch的所有poem中最长的poem的长度\n",
        "        length = max(map(len, batches))\n",
        "        # 填充一个这么大小的空batch，空的地方放空格对应的index标号\n",
        "        x_data = np.full((batch_size, length), word_to_int[' '], np.int32)\n",
        "        for row in range(batch_size):\n",
        "            x_data[row, :len(batches[row])] = batches[row]\n",
        "        y_data = np.copy(x_data)\n",
        "        y_data[:, :-1] = x_data[:, 1:]\n",
        "        \"\"\"\n",
        "        x_data             y_data\n",
        "        [6,2,4,6,9]       [2,4,6,9,9]\n",
        "        [1,4,2,8,5]       [4,2,8,5,5]\n",
        "        \"\"\"\n",
        "        x_batches.append(x_data)\n",
        "        y_batches.append(y_data)\n",
        "    return x_batches, y_batches\n",
        "\n",
        "def to_word(predict, vocabs):# 预测的结果转化成汉字\n",
        "    sample = np.argmax(predict)\n",
        "    if sample > len(vocabs):\n",
        "        sample = len(vocabs) - 1\n",
        "    return vocabs[sample]\n",
        "def pretty_print_poem(poem):#  令打印的结果更工整\n",
        "    poem_sentences = poem.split('。')\n",
        "    for s in poem_sentences:\n",
        "        if s != '' and len(s) > 10:\n",
        "            print(s + '。')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiGZ1z3x-Yb8",
        "colab_type": "text"
      },
      "source": [
        "### 主函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InjsVgsh-Yb9",
        "colab_type": "code",
        "outputId": "879bde59-e4bc-4e26-9754-ee50fd5c68da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "source": [
        "#print('[INFO] train tang poem...')\n",
        "#run_training() # 训练模型\n",
        "#print('[INFO] write tang poem...')\n",
        "poem2 = gen_poem('景')# 生成诗歌\n",
        "print(\"#\" * 25)\n",
        "pretty_print_poem(poem2)\n",
        "print('#' * 25)\n",
        "#训练模型时间比较长，训练模型完成后每次生成诗歌的时，不需要再次训练 ，可以注销上面的 run_training()。生成部分执行速度很快"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading corpus from poems.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwhOiLeA-YcC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "0f27a68c-c4a1-491d-8876-c749b24579b9"
      },
      "source": [
        "poem2 = gen_poem('景')# 生成诗歌\n",
        "print(\"#\" * 25)\n",
        "pretty_print_poem(poem2)\n",
        "print('#' * 25)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading corpus from poems.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
        }
      ]
    }
  ]
}
